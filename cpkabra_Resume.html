
<html>
<head>
<meta charset="UTF-8">
<title>Modern CV - Chandraprakash Kabra</title>
<style>
body {
    font-family: 'Segoe UI', sans-serif;
    background: #eef1f5;
    margin: 0;
    padding: 40px;
}
.cv-container {
    max-width: 1000px;
    margin: auto;
    background: #ffffff;
    border-radius: 14px;
    overflow: hidden;
    box-shadow: 0 10px 35px rgba(0,0,0,0.1);
    display: grid;
    grid-template-columns: 32% 68%;
}
.sidebar {
    background: #003366;
    color: white;
    padding: 30px;
}
.sidebar h2 {
    border-bottom: 1px solid rgba(255,255,255,0.4);
    padding-bottom: 8px;
}
.main {
    padding: 40px;
}
.section-title {
    font-size: 20px;
    color: #003366;
    margin-top: 30px;
    border-left: 4px solid #003366;
    padding-left: 10px;
}
p {
    line-height: 1.6;
}
</style>
</head>
<body>
<div class="cv-container">

<div class="sidebar">
<h1>Chandraprakash Kabra</h1>
<p><b>Mobile:</b> +91 9033588658 / 8320302852</p>
<p><b>Email:</b> chandraprakashkabra@gmail.com</p>
<p><b>LinkedIn:</b> linkedin.com/in/cpkabra</p>
<p><b>GitHub:</b> github.com/cpkabra</p>
<p><b>Address:</b><br>B2-704, Sampanna Homes, Manjari BK, Pune 412307</p>

<h2>Technical Skills</h2>
<p><b>Languages:</b> Scala, Python, C#</p>
<p><b>Databases:</b> SQL Server, Postgres</p>
<p><b>Tools:</b> Airflow, Databricks, Hadoop, Hive</p>
<p><b>Other:</b> HTML, CSS, JavaScript, Bootstrap, AJAX</p>
</div>

<div class="main">
<h2 class="section-title">Profile Summary</h2>
<p>15 years of experience in software development including 6 years in Big Data ecosystem. Strong expertise in Apache Spark, Hadoop, Airflow, Azure Cloud, Databricks, and ETL pipelines. Excellent debugging skills, agile methodology adoption, and team leadership.</p>

<h2 class="section-title">Experience</h2>
<p><b>Associate Director – UBS, Pune</b> (Feb 2023 – Present)</p>
<p><b>Lead Software Developer – S&P Global (L&T), Ahmedabad</b> (Nov 2017 – Jan 2023)</p>
<p><b>Senior Software Engineer – Cybage Software</b> (Feb 2015 – Oct 2017)</p>
<p><b>Software Engineer – TheBeastApps</b> (Feb 2014 – Feb 2015)</p>
<p><b>Software Engineer – ITXGEN</b> (Jul 2013 – Jan 2014)</p>
<p><b>Software Engineer – Pulse Solutions</b> (Jan 2013 – Jun 2013)</p>
<p><b>Software Engineer – E-Connect Solutions</b> (Sep 2010 – Sep 2012)</p>

<h2 class="section-title">Projects</h2>

<p>Implemented many data pipeline using Apache Spark, Hadoop, Airflow, Hive, python, Scala,</p>
<p>Databricks, Nifi.</p>
<p>Following Agile Methodology.</p>
<p>Strong Debugging Skills with Knowledge on good coding Practices.</p>
<p>Excellent communication skills and leading the team of 5 developers as Dev lead.</p>
<p>Worked on many ETL projects.</p>
<p>Having 4 years of experience in Azure Cloud i.e. data factory, data lake, AWS Cloud i.e. EC2, S3 as well</p>
<h3 style='color:#222;margin-top:25px;'>PROJECT TITLE: ESG Pipelines</h3>
<p>PROJECT DESCRIPTION: There are many pipelines which is scheduled using Azure data factory and ingest data from</p>
<p>raw zone to curated to master to cosmos.</p>
<p>We have dependency on acquisitions team who is responsible for data in raw then after we take from raw</p>
<p>and created generic framework so by using that we can process data to curated and then master and then</p>
<p>if required then sending to cosmos as well.</p>
<p>Another API team is there they wrote API to read data from cosmos and consumer consume those API.</p>
<p>Technology: Apache Spark, Scala, Azure Databricks, Azure Datafactory, Azure Datalake, Cosmos db</p>
<p>Contribution:</p>
<p>Created Many pipelines for data ingestion from raw to till cosmos</p>
<p>Involved in monthly release and prod support.</p>
<p>Creating tech debt story and assisting team members</p>
<h3 style='color:#222;margin-top:25px;'>PROJECT TITLE: Ownershiptransformation to XpressFeed</h3>
<p>PROJECT DESCRIPTION: Huge ownership data volume are generating daily for and company in sql server.</p>
<p>We need to move those data into hive through spark and do some preprocessing, complex calculation, and</p>
<p>generate difference for each table and finally export it into feed sql server.</p>
<p>Over here we are entertaining 30 tables in which some of them are having huge records like more than 400 Millions</p>
<p>and we are successfully completing our workflow in 4 hours.</p>
<p>Technology: Apache Spark, Scala, Python, Hive, Airflow, Sql Server</p>
<p>Contribution:</p>
<p>Involved in requirement gathering and architecture design.</p>
<p>Wrote a generic logic from diff generation and export to sql.</p>
<p>Created DAGs and used many airflow operators</p>
<p>Involved from code peer review to till production deployment.</p>
<h3 style='color:#222;margin-top:25px;'>PROJECT TITLE: Data movement and comparison for analytics</h3>
<p>PROJECT DESCRIPTION: Source data are in sql server and one team is generating one incremental changes file in</p>
<p>parquet format and putting on s3 location. Folder structure are month wise so they are putting entire month file in</p>
<p>single folder with date wise. Now we need to read parquet file, remove duplicate record based on primary key</p>
<p>which is combination of multiple columns and saving it as a delta table. Over here there are multiple table so we</p>
<p>wrote generic code to it will work for all the tables. We have inserted data into delta table based after validation</p>
<p>and filter.</p>
<p>Now data are available in delta table. We need to filter out numeric columns and do aggregation and i.e. sum,</p>
<p>min, max and count. Now we can compare delta data with source data[they are putting one file daily which</p>
<p>having aggregation for daily records].If source and target are identical then it is ok if not then we need to</p>
<p>send email alert.</p>
<p>Technology: Apache Spark, Python, Hive, Postgres</p>
<p>Contribution:</p>
<p>Define the process, design flow chart for entire application.</p>
<p>Write a generic code to generate tables and wrote logic to do comparison.</p>
<p>Involved in code peer review.</p>
<h3 style='color:#222;margin-top:25px;'>PROJECT TITLE: Electronic quarterly report[EQR]</h3>
<p>PROJECT DESCRIPTION: Every quarter one big zip file is generated, so we need to pick that file from ftp location and</p>
<p>unzip it and move to s3. Reading data from s3 and saving to hive table with minimal transformation by adding</p>
<p>some additional columns. In further step we are mapping those raw data with some lookup tables from sql server.</p>
<p>Next step is load to clean means we are saving valid data in one table and unvalid data into another table.</p>
<p>Then after clean hive table are moved to postgres database.</p>
<p>Technology: Apache Spark, Python, Hive, Postgres</p>
<p>Contribution:</p>
<p>Coordinates with Infrastructure team for enough recourse.</p>
<p>Involved in Load to map steps mostly.</p>
<h3 style='color:#222;margin-top:25px;'>PROJECT TITLE: Trucost Aggregator [Apache Nifi Project]</h3>
<p>PROJECT DESCRIPTION: Whatever data is updating through loader we need to do aggregation on around 30 fields.</p>
<p>Changes in database are capture through CDC and sending all those changes through Kafka messages. In Nifi</p>
<p>we have used ConsumeKafka Processor to read all those messages and filtered through RouteAttrubute Processor</p>
<p>and sending further to process.</p>
<p>Actually we are getting changes on real time but our requirement is to run aggregation once in a day so are storing</p>
<p>one unique key into cache server [DistributedMapCache].</p>
<p>Before putting value into cache we are also check in Duplicate, if it is not in cache then are storing and applied</p>
<p>CRON scheduling on FetchDistrubuteCache and running this Processor on 24 hours and removing this key from</p>
<p>cache server once it is sent for processing.</p>
<p>We are making many OData call to prepare request format to make an API call and once we receive response</p>
<p>from API then we are converting it into database complaint format and storing data into database.</p>
<p>Technology: Apache Nifi, Kafka, Groovy, Python, Odata, Kibana logging</p>
<p>Contribution:</p>
<p>Coordinates with Infrastructure team for enough recourse.</p>
<p>Involved in Load to map steps mostly.</p>
<p>AMIE (2014) (BE equivalent) in Computer Science with 7.2 CGPA.</p>
<p>Diploma in IT (2010) from Vidya Bhawan Polytechnic College, Udaipur (Rajasthan)  with 75.30%</p>
<p>HSC from Rajasthan board with 67.69% score (2007).</p>
<p>SSC from Rajasthan board with 81.83% score (2005).</p>
<p>AI-100: Azure AI Engineer Associate.</p>
<p>AZ-204: Microsoft Azure Developer Associate</p>
<p>Apache Airflow Fundamentals</p>
<p>Databricks Lakehouse Fundamentals</p>
<p>Databricks Certified Associate Developer for Apache Spark 3.0</p>

<h2 class="section-title">Education</h2>
<p>AMIE (BE Equivalent), Computer Science – 7.2 CGPA (2014)</p>
<p>Diploma in IT – 75.30% (2010)</p>
<p>HSC – 67.69% (2007)</p>
<p>SSC – 81.83% (2005)</p>

<h2 class="section-title">Certificates</h2>
<p>AI-100: Azure AI Engineer Associate</p>
<p>AZ-900: Azure Fundamentals</p>
<p>AZ-204: Azure Developer Associate</p>
<p>Databricks Spark 3.0 Developer Certification</p>

</div>
</div>
</body>
</html>
